{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook uses a Session Event Dataset from E-Commerce Website (https://www.kaggle.com/mkechinov/ecommerce-behavior-data-from-multi-category-store and https://rees46.com/) to build an Outlier Detection based on an Autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json             2.0.9\n",
      "tensorflow.keras 2.4.0\n",
      "tensorflow       2.4.0\n",
      "autopep8         1.5.4\n",
      "numpy            1.19.5\n",
      "mlflow           1.13.1\n",
      "pandas           1.0.5\n",
      "tensorflow_hub   0.9.0\n",
      "CPython 3.7.4\n",
      "IPython 7.8.0\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow_hub as hub\n",
    "from itertools import product\n",
    "\n",
    "# enable gpu growth if gpu is available\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "for device in gpu_devices:\n",
    "    tf.config.experimental.set_memory_growth(device, True)\n",
    "    \n",
    "%load_ext watermark\n",
    "%watermark -v -iv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Registry and Tracking URI for MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this registry uri when mlflow is created by docker container with a mysql db backend\n",
    "#registry_uri = os.path.expandvars('mysql+pymysql://${MYSQL_USER}:${MYSQL_PASSWORD}@localhost:3306/${MYSQL_DATABASE}')\n",
    "\n",
    "# Use this registry uri when mlflow is running locally by the command:\n",
    "# \"mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./mlruns --host 0.0.0.0\"\n",
    "registry_uri = 'sqlite:///mlflow.db'\n",
    "\n",
    "tracking_uri = 'http://localhost:5000'\n",
    "\n",
    "mlflow.tracking.set_registry_uri(registry_uri)\n",
    "mlflow.tracking.set_tracking_uri(tracking_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yoochoose-clicks.dat - Click events. Each record/line in the file has the following fields:\n",
    "1. Session ID – the id of the session. In one session there are one or many clicks.\n",
    "2. Timestamp – the time when the click occurred.\n",
    "3. Item ID – the unique identifier of the item.\n",
    "4. Category – the category of the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique Items: 38515\n",
      "Mean: 284.77105468660056\n",
      "Std: 349.4674023158121\n",
      "Sessions: (61296,)\n",
      "Unique Products: (38515,)\n",
      "Unique category_code: (134,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding_0</th>\n",
       "      <th>embedding_1</th>\n",
       "      <th>embedding_2</th>\n",
       "      <th>embedding_3</th>\n",
       "      <th>embedding_4</th>\n",
       "      <th>embedding_5</th>\n",
       "      <th>embedding_6</th>\n",
       "      <th>embedding_7</th>\n",
       "      <th>embedding_8</th>\n",
       "      <th>embedding_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding_44</th>\n",
       "      <th>embedding_45</th>\n",
       "      <th>embedding_46</th>\n",
       "      <th>embedding_47</th>\n",
       "      <th>embedding_48</th>\n",
       "      <th>embedding_49</th>\n",
       "      <th>price_standardized</th>\n",
       "      <th>user_session</th>\n",
       "      <th>Product</th>\n",
       "      <th>product_id_mapped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.047610</td>\n",
       "      <td>-0.125734</td>\n",
       "      <td>-0.053261</td>\n",
       "      <td>0.196848</td>\n",
       "      <td>-0.016433</td>\n",
       "      <td>0.049773</td>\n",
       "      <td>0.012852</td>\n",
       "      <td>0.263229</td>\n",
       "      <td>-0.195415</td>\n",
       "      <td>0.327227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106797</td>\n",
       "      <td>-0.214412</td>\n",
       "      <td>0.090539</td>\n",
       "      <td>0.104421</td>\n",
       "      <td>0.061444</td>\n",
       "      <td>-0.008996</td>\n",
       "      <td>-0.373028</td>\n",
       "      <td>0000afb3-2d30-4b52-84ec-07c6617efd37</td>\n",
       "      <td>light</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047610</td>\n",
       "      <td>-0.125734</td>\n",
       "      <td>-0.053261</td>\n",
       "      <td>0.196848</td>\n",
       "      <td>-0.016433</td>\n",
       "      <td>0.049773</td>\n",
       "      <td>0.012852</td>\n",
       "      <td>0.263229</td>\n",
       "      <td>-0.195415</td>\n",
       "      <td>0.327227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106797</td>\n",
       "      <td>-0.214412</td>\n",
       "      <td>0.090539</td>\n",
       "      <td>0.104421</td>\n",
       "      <td>0.061444</td>\n",
       "      <td>-0.008996</td>\n",
       "      <td>-0.373028</td>\n",
       "      <td>0000afb3-2d30-4b52-84ec-07c6617efd37</td>\n",
       "      <td>light</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.047610</td>\n",
       "      <td>-0.125734</td>\n",
       "      <td>-0.053261</td>\n",
       "      <td>0.196848</td>\n",
       "      <td>-0.016433</td>\n",
       "      <td>0.049773</td>\n",
       "      <td>0.012852</td>\n",
       "      <td>0.263229</td>\n",
       "      <td>-0.195415</td>\n",
       "      <td>0.327227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106797</td>\n",
       "      <td>-0.214412</td>\n",
       "      <td>0.090539</td>\n",
       "      <td>0.104421</td>\n",
       "      <td>0.061444</td>\n",
       "      <td>-0.008996</td>\n",
       "      <td>0.068930</td>\n",
       "      <td>0000b83c-9b26-4881-8bca-e20d460f4194</td>\n",
       "      <td>light</td>\n",
       "      <td>887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.047610</td>\n",
       "      <td>-0.125734</td>\n",
       "      <td>-0.053261</td>\n",
       "      <td>0.196848</td>\n",
       "      <td>-0.016433</td>\n",
       "      <td>0.049773</td>\n",
       "      <td>0.012852</td>\n",
       "      <td>0.263229</td>\n",
       "      <td>-0.195415</td>\n",
       "      <td>0.327227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106797</td>\n",
       "      <td>-0.214412</td>\n",
       "      <td>0.090539</td>\n",
       "      <td>0.104421</td>\n",
       "      <td>0.061444</td>\n",
       "      <td>-0.008996</td>\n",
       "      <td>0.289895</td>\n",
       "      <td>0000b83c-9b26-4881-8bca-e20d460f4194</td>\n",
       "      <td>light</td>\n",
       "      <td>605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.047610</td>\n",
       "      <td>-0.125734</td>\n",
       "      <td>-0.053261</td>\n",
       "      <td>0.196848</td>\n",
       "      <td>-0.016433</td>\n",
       "      <td>0.049773</td>\n",
       "      <td>0.012852</td>\n",
       "      <td>0.263229</td>\n",
       "      <td>-0.195415</td>\n",
       "      <td>0.327227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106797</td>\n",
       "      <td>-0.214412</td>\n",
       "      <td>0.090539</td>\n",
       "      <td>0.104421</td>\n",
       "      <td>0.061444</td>\n",
       "      <td>-0.008996</td>\n",
       "      <td>-0.291904</td>\n",
       "      <td>0000f7c4-8836-4507-82a1-8a10de3fb1b2</td>\n",
       "      <td>light</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369299</th>\n",
       "      <td>0.174397</td>\n",
       "      <td>-0.204014</td>\n",
       "      <td>-0.175919</td>\n",
       "      <td>0.105906</td>\n",
       "      <td>-0.205940</td>\n",
       "      <td>-0.210022</td>\n",
       "      <td>0.224095</td>\n",
       "      <td>0.210598</td>\n",
       "      <td>-0.162613</td>\n",
       "      <td>0.150693</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084346</td>\n",
       "      <td>0.071817</td>\n",
       "      <td>-0.129293</td>\n",
       "      <td>-0.176801</td>\n",
       "      <td>0.100340</td>\n",
       "      <td>0.119850</td>\n",
       "      <td>-0.630791</td>\n",
       "      <td>fffdfd5e-126c-409f-9c16-8224f22cb60b</td>\n",
       "      <td>cooler</td>\n",
       "      <td>7953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369300</th>\n",
       "      <td>0.047610</td>\n",
       "      <td>-0.125734</td>\n",
       "      <td>-0.053261</td>\n",
       "      <td>0.196848</td>\n",
       "      <td>-0.016433</td>\n",
       "      <td>0.049773</td>\n",
       "      <td>0.012852</td>\n",
       "      <td>0.263229</td>\n",
       "      <td>-0.195415</td>\n",
       "      <td>0.327227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106797</td>\n",
       "      <td>-0.214412</td>\n",
       "      <td>0.090539</td>\n",
       "      <td>0.104421</td>\n",
       "      <td>0.061444</td>\n",
       "      <td>-0.008996</td>\n",
       "      <td>-0.511696</td>\n",
       "      <td>fffe34dd-9537-4991-9f12-d81f1dda91cb</td>\n",
       "      <td>light</td>\n",
       "      <td>844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369301</th>\n",
       "      <td>0.047610</td>\n",
       "      <td>-0.125734</td>\n",
       "      <td>-0.053261</td>\n",
       "      <td>0.196848</td>\n",
       "      <td>-0.016433</td>\n",
       "      <td>0.049773</td>\n",
       "      <td>0.012852</td>\n",
       "      <td>0.263229</td>\n",
       "      <td>-0.195415</td>\n",
       "      <td>0.327227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106797</td>\n",
       "      <td>-0.214412</td>\n",
       "      <td>0.090539</td>\n",
       "      <td>0.104421</td>\n",
       "      <td>0.061444</td>\n",
       "      <td>-0.008996</td>\n",
       "      <td>-0.511696</td>\n",
       "      <td>fffe34dd-9537-4991-9f12-d81f1dda91cb</td>\n",
       "      <td>light</td>\n",
       "      <td>844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369302</th>\n",
       "      <td>0.047610</td>\n",
       "      <td>-0.125734</td>\n",
       "      <td>-0.053261</td>\n",
       "      <td>0.196848</td>\n",
       "      <td>-0.016433</td>\n",
       "      <td>0.049773</td>\n",
       "      <td>0.012852</td>\n",
       "      <td>0.263229</td>\n",
       "      <td>-0.195415</td>\n",
       "      <td>0.327227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106797</td>\n",
       "      <td>-0.214412</td>\n",
       "      <td>0.090539</td>\n",
       "      <td>0.104421</td>\n",
       "      <td>0.061444</td>\n",
       "      <td>-0.008996</td>\n",
       "      <td>-0.459760</td>\n",
       "      <td>fffe34dd-9537-4991-9f12-d81f1dda91cb</td>\n",
       "      <td>light</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369303</th>\n",
       "      <td>0.047610</td>\n",
       "      <td>-0.125734</td>\n",
       "      <td>-0.053261</td>\n",
       "      <td>0.196848</td>\n",
       "      <td>-0.016433</td>\n",
       "      <td>0.049773</td>\n",
       "      <td>0.012852</td>\n",
       "      <td>0.263229</td>\n",
       "      <td>-0.195415</td>\n",
       "      <td>0.327227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.106797</td>\n",
       "      <td>-0.214412</td>\n",
       "      <td>0.090539</td>\n",
       "      <td>0.104421</td>\n",
       "      <td>0.061444</td>\n",
       "      <td>-0.008996</td>\n",
       "      <td>-0.459760</td>\n",
       "      <td>fffe34dd-9537-4991-9f12-d81f1dda91cb</td>\n",
       "      <td>light</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>369304 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        embedding_0  embedding_1  embedding_2  embedding_3  embedding_4  \\\n",
       "0          0.047610    -0.125734    -0.053261     0.196848    -0.016433   \n",
       "1          0.047610    -0.125734    -0.053261     0.196848    -0.016433   \n",
       "2          0.047610    -0.125734    -0.053261     0.196848    -0.016433   \n",
       "3          0.047610    -0.125734    -0.053261     0.196848    -0.016433   \n",
       "4          0.047610    -0.125734    -0.053261     0.196848    -0.016433   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "369299     0.174397    -0.204014    -0.175919     0.105906    -0.205940   \n",
       "369300     0.047610    -0.125734    -0.053261     0.196848    -0.016433   \n",
       "369301     0.047610    -0.125734    -0.053261     0.196848    -0.016433   \n",
       "369302     0.047610    -0.125734    -0.053261     0.196848    -0.016433   \n",
       "369303     0.047610    -0.125734    -0.053261     0.196848    -0.016433   \n",
       "\n",
       "        embedding_5  embedding_6  embedding_7  embedding_8  embedding_9  ...  \\\n",
       "0          0.049773     0.012852     0.263229    -0.195415     0.327227  ...   \n",
       "1          0.049773     0.012852     0.263229    -0.195415     0.327227  ...   \n",
       "2          0.049773     0.012852     0.263229    -0.195415     0.327227  ...   \n",
       "3          0.049773     0.012852     0.263229    -0.195415     0.327227  ...   \n",
       "4          0.049773     0.012852     0.263229    -0.195415     0.327227  ...   \n",
       "...             ...          ...          ...          ...          ...  ...   \n",
       "369299    -0.210022     0.224095     0.210598    -0.162613     0.150693  ...   \n",
       "369300     0.049773     0.012852     0.263229    -0.195415     0.327227  ...   \n",
       "369301     0.049773     0.012852     0.263229    -0.195415     0.327227  ...   \n",
       "369302     0.049773     0.012852     0.263229    -0.195415     0.327227  ...   \n",
       "369303     0.049773     0.012852     0.263229    -0.195415     0.327227  ...   \n",
       "\n",
       "        embedding_44  embedding_45  embedding_46  embedding_47  embedding_48  \\\n",
       "0           0.106797     -0.214412      0.090539      0.104421      0.061444   \n",
       "1           0.106797     -0.214412      0.090539      0.104421      0.061444   \n",
       "2           0.106797     -0.214412      0.090539      0.104421      0.061444   \n",
       "3           0.106797     -0.214412      0.090539      0.104421      0.061444   \n",
       "4           0.106797     -0.214412      0.090539      0.104421      0.061444   \n",
       "...              ...           ...           ...           ...           ...   \n",
       "369299      0.084346      0.071817     -0.129293     -0.176801      0.100340   \n",
       "369300      0.106797     -0.214412      0.090539      0.104421      0.061444   \n",
       "369301      0.106797     -0.214412      0.090539      0.104421      0.061444   \n",
       "369302      0.106797     -0.214412      0.090539      0.104421      0.061444   \n",
       "369303      0.106797     -0.214412      0.090539      0.104421      0.061444   \n",
       "\n",
       "        embedding_49  price_standardized  \\\n",
       "0          -0.008996           -0.373028   \n",
       "1          -0.008996           -0.373028   \n",
       "2          -0.008996            0.068930   \n",
       "3          -0.008996            0.289895   \n",
       "4          -0.008996           -0.291904   \n",
       "...              ...                 ...   \n",
       "369299      0.119850           -0.630791   \n",
       "369300     -0.008996           -0.511696   \n",
       "369301     -0.008996           -0.511696   \n",
       "369302     -0.008996           -0.459760   \n",
       "369303     -0.008996           -0.459760   \n",
       "\n",
       "                                user_session  Product  product_id_mapped  \n",
       "0       0000afb3-2d30-4b52-84ec-07c6617efd37    light                 27  \n",
       "1       0000afb3-2d30-4b52-84ec-07c6617efd37    light                 27  \n",
       "2       0000b83c-9b26-4881-8bca-e20d460f4194    light                887  \n",
       "3       0000b83c-9b26-4881-8bca-e20d460f4194    light                605  \n",
       "4       0000f7c4-8836-4507-82a1-8a10de3fb1b2    light               1506  \n",
       "...                                      ...      ...                ...  \n",
       "369299  fffdfd5e-126c-409f-9c16-8224f22cb60b   cooler               7953  \n",
       "369300  fffe34dd-9537-4991-9f12-d81f1dda91cb    light                844  \n",
       "369301  fffe34dd-9537-4991-9f12-d81f1dda91cb    light                844  \n",
       "369302  fffe34dd-9537-4991-9f12-d81f1dda91cb    light                  8  \n",
       "369303  fffe34dd-9537-4991-9f12-d81f1dda91cb    light                  8  \n",
       "\n",
       "[369304 rows x 54 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for chunk in pd.read_table(\"2019-Dec.csv\",\n",
    "                           sep=\",\", header=0,\n",
    "                           infer_datetime_format=True, low_memory=False, chunksize=500000):\n",
    "    # Filter out other event types than 'view'\n",
    "    chunk = chunk[chunk['event_type'] == 'view']\n",
    "    # Filter out missing 'category_code' rows\n",
    "    chunk = chunk[chunk['category_code'].isna() == False]\n",
    "    chunk.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Filter out all Sessions of length 1\n",
    "    count_sessions = chunk.groupby('user_session').count()\n",
    "    window_length = count_sessions.max()[0]\n",
    "    unique_sessions = [count_sessions.index[i] for i in range(\n",
    "        count_sessions.shape[0]) if count_sessions.iloc[i, 0] == 1]\n",
    "    chunk = chunk[chunk['user_session'].isin(unique_sessions) == False]\n",
    "    chunk.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Text embedding based on https://tfhub.dev/google/nnlm-en-dim50/2\n",
    "    last_category = []\n",
    "    for i, el in enumerate(chunk['category_code']):\n",
    "        last_category.append(el.split('.')[-1])\n",
    "    chunk['Product'] = last_category\n",
    "    embed = hub.load(\"https://tfhub.dev/google/nnlm-en-dim50/2\")\n",
    "    embeddings = embed(chunk['Product'].tolist())\n",
    "    for dim in range(embeddings.shape[1]):\n",
    "        chunk['embedding_'+str(dim)] = embeddings[:, dim]        \n",
    "     \n",
    "    # Create Item IDs starting from value 1 for Embeddings and One Hot Layer\n",
    "    unique_items = pd.unique(chunk['product_id'])\n",
    "    print('Number of unique Items:', unique_items.shape[0])\n",
    "    dict_items = dict(zip(unique_items, [i+1 for i in range(unique_items.shape[0])]))\n",
    "    chunk['product_id_mapped'] = chunk['product_id'].map(dict_items)\n",
    "    d = pd.DataFrame.from_records(data=list(dict_items.items()), columns=['Item_ID', 'Mapped_ID'])\n",
    "    # map product_id to category_code\n",
    "    d['category_code'] = [chunk[chunk['product_id'] == i]['category_code'].values[0] for i in d['Item_ID']]\n",
    "    d.to_csv('ID_Mapping.csv')\n",
    "\n",
    "    # Standardization\n",
    "    mean = chunk['price'].mean(axis=0)\n",
    "    print('Mean:', mean)\n",
    "    std = chunk['price'].std(axis=0)\n",
    "    print('Std:', std)\n",
    "    chunk['price_standardized'] = (chunk['price'] - mean) / std\n",
    "\n",
    "    \n",
    "    chunk.sort_values(by=['user_session', 'event_time'], inplace=True)\n",
    "    chunk['price_standardized'] = chunk['price_standardized'].astype('float32')\n",
    "    chunk['product_id_mapped'] = chunk['product_id_mapped'].astype('int32')\n",
    "    chunk.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    \n",
    "    print('Sessions:', pd.unique(chunk['user_session']).shape)\n",
    "    print('Unique Products:',pd.unique(chunk['product_id']).shape)\n",
    "    print('Unique category_code:',pd.unique(chunk['category_code']).shape)\n",
    "    \n",
    "    columns = ['embedding_'+str(i) for i in range(embeddings.shape[1])]\n",
    "    columns.append('price_standardized')\n",
    "    columns.append('user_session')\n",
    "    columns.append('Product')\n",
    "    columns.append('product_id_mapped')\n",
    "    \n",
    "    df = chunk[columns]\n",
    "    break\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice Sessions from the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sessions = []\n",
    "list_last_clicked = []\n",
    "list_last_clicked_temp = []\n",
    "current_id = df.loc[0, 'user_session']\n",
    "current_index = 0\n",
    "\n",
    "columns = ['embedding_'+str(i) for i in range(embeddings.shape[1])]\n",
    "columns.append('price_standardized')\n",
    "columns.insert(0, 'product_id_mapped')\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    if df.loc[i, 'user_session'] != current_id:\n",
    "        list_sessions.append(df.loc[current_index:i-2, columns])\n",
    "        list_last_clicked.append(df.loc[i-1, 'product_id_mapped'])\n",
    "        list_last_clicked_temp.append(df.loc[i-1, columns])\n",
    "        current_id = df.loc[i, 'user_session']\n",
    "        current_index = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice Sessions if label and last product from session is the same\n",
    "Example:\n",
    "- From: session: [ 1506  1506 11410 11410  2826  2826], ground truth: 2826\n",
    "- To: session: [ 1506  1506 11410 11410], ground truth: 2826"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length before 61295\n",
      "Length after 45916\n"
     ]
    }
   ],
   "source": [
    "print(\"Length before\", len(list_sessions))\n",
    "list_sessions_processed = []\n",
    "list_last_clicked_processed = []\n",
    "list_session_processed_autoencoder = []\n",
    "\n",
    "for i, session in enumerate(list_sessions):\n",
    "    if session['product_id_mapped'].values[-1] == list_last_clicked[i]:\n",
    "        mask = session['product_id_mapped'].values == list_last_clicked[i]\n",
    "        if session[~mask].shape[0] > 0:\n",
    "            list_sessions_processed.append(session[~mask])\n",
    "            list_last_clicked_processed.append(list_last_clicked[i])\n",
    "            list_session_processed_autoencoder.append(np.concatenate(\n",
    "                [session[~mask].values, list_last_clicked_temp[i].values[np.newaxis, :]], axis=0))\n",
    "    else:\n",
    "        list_sessions_processed.append(session)\n",
    "        list_last_clicked_processed.append(list_last_clicked[i])\n",
    "        list_session_processed_autoencoder.append(np.concatenate(\n",
    "                [session.values, list_last_clicked_temp[i].values[np.newaxis, :]], axis=0))\n",
    "\n",
    "print(\"Length after\", len(list_sessions_processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_output_features 38515\n",
      "n_unique_input_ids 38515\n",
      "window_length 207\n",
      "n_input_features 1\n"
     ]
    }
   ],
   "source": [
    "# Pad all Sessions with 0. Embedding Layer and LSTM will use Masking to ignore zeros.\n",
    "list_sessions_padded = []\n",
    "\n",
    "for np_array in list_session_processed_autoencoder:\n",
    "    result = np.zeros((window_length, 1), dtype=np.float32)\n",
    "\n",
    "    result[:np_array.shape[0],:1] = np_array[:,:1]\n",
    "    list_sessions_padded.append(result)\n",
    "\n",
    "\n",
    "# Save the results, because the slicing can take some time\n",
    "np.save('list_sessions_padded_autoencoder.npy', list_sessions_padded)\n",
    "\n",
    "sessions_padded = np.array(list_sessions_padded)\n",
    "\n",
    "n_output_features = int(sessions_padded.max())\n",
    "n_unique_input_ids = int(sessions_padded.max())\n",
    "window_length = sessions_padded.shape[1]\n",
    "n_input_features = sessions_padded.shape[2]\n",
    "print(\"n_output_features\", n_output_features)\n",
    "print(\"n_unique_input_ids\", n_unique_input_ids)\n",
    "print(\"window_length\", window_length)\n",
    "print(\"n_input_features\", n_input_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: Start here if the preprocessing was already executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45916, 207, 1)\n"
     ]
    }
   ],
   "source": [
    "sessions_padded = np.load('list_sessions_padded_autoencoder.npy')\n",
    "print(sessions_padded.shape)\n",
    "n_output_features = int(sessions_padded.max())\n",
    "n_unique_input_ids = int(sessions_padded.max())\n",
    "window_length = sessions_padded.shape[1]\n",
    "n_input_features = sessions_padded.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Hyperparameter\n",
    "Dictionary with different hyperparameters to train on.\n",
    "MLflow will track those in a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'hidden_layer_size': 500,\n",
       "  'batch_size': 10,\n",
       "  'embedding_dim': 200,\n",
       "  'window_length': 207,\n",
       "  'dropout_fc': 0.0,\n",
       "  'n_output_features': 38515,\n",
       "  'n_input_features': 1}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_dic = {'hidden_layer_size': [500],\n",
    "                   'batch_size': [10],\n",
    "                   'embedding_dim': [200],\n",
    "                   'window_length': [window_length],\n",
    "                   'dropout_fc': [0.0], #0.2\n",
    "                   'n_output_features': [n_output_features],\n",
    "                   'n_input_features': [n_input_features]}\n",
    "\n",
    "# Cartesian product\n",
    "grid_search_param = [dict(zip(grid_search_dic, v)) for v in product(*grid_search_dic.values())]\n",
    "grid_search_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Autoencoder in functional API\n",
    "- Input: x rows (time steps) of Item IDs in a Session\n",
    "- Output: reconstructed Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(window_length=50,\n",
    "                      units_lstm_layer=100,\n",
    "                      n_unique_input_ids=0,\n",
    "                      embedding_dim=200,\n",
    "                      n_input_features=1,\n",
    "                      n_output_features=3,\n",
    "                      dropout_rate=0.1):\n",
    "\n",
    "    inputs = keras.layers.Input(\n",
    "        shape=[window_length, n_input_features], dtype=np.float32)\n",
    "\n",
    "    # Encoder\n",
    "    # Embedding Layer\n",
    "    embedding_layer = tf.keras.layers.Embedding(\n",
    "        n_unique_input_ids+1, embedding_dim, input_length=window_length)  # , mask_zero=True)\n",
    "    embeddings = embedding_layer(inputs[:, :, 0])\n",
    "\n",
    "    mask = inputs[:, :, 0] != 0\n",
    "\n",
    "    # LSTM Layer 1\n",
    "    lstm1_output, lstm1_state_h, lstm1_state_c = keras.layers.LSTM(units=units_lstm_layer, return_state=True,\n",
    "                                                                   return_sequences=True)(embeddings, mask=mask)\n",
    "    lstm1_state = [lstm1_state_h, lstm1_state_c]\n",
    "\n",
    "    # Decoder\n",
    "    # input: lstm1_state_c, lstm1_state_h\n",
    "    decoder_state_c = lstm1_state_c\n",
    "    decoder_state_h = lstm1_state_h\n",
    "    decoder_outputs = tf.expand_dims(lstm1_state_h, 1)\n",
    "\n",
    "    list_states = []\n",
    "    decoder_layer = keras.layers.LSTM(\n",
    "        units=units_lstm_layer, return_state=True, return_sequences=True, unroll=False)\n",
    "    for i in range(window_length):\n",
    "        decoder_outputs, decoder_state_h, decoder_state_c = decoder_layer(decoder_outputs,\n",
    "                                                                          initial_state=[decoder_state_h,\n",
    "                                                                                         decoder_state_c])\n",
    "        list_states.append(decoder_state_h)\n",
    "    stacked = tf.stack(list_states, axis=1)\n",
    "\n",
    "    fc_layer = tf.keras.layers.Dense(\n",
    "        n_output_features+1, kernel_initializer='he_normal', dtype=tf.float32)\n",
    "\n",
    "    fc_layer_output = tf.keras.layers.TimeDistributed(fc_layer)(\n",
    "        stacked, mask=mask)\n",
    "\n",
    "    mask_softmax = tf.tile(tf.expand_dims(mask, axis=2),\n",
    "                           [1, 1, n_output_features+1])\n",
    "\n",
    "    softmax = tf.keras.layers.Softmax(axis=2)(\n",
    "        fc_layer_output, mask=mask_softmax)\n",
    "\n",
    "    model = keras.models.Model(inputs=[inputs],\n",
    "                               outputs=[softmax])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Numpy Array to tf.data.Dataset for better training performance\n",
    "The function will return a zipped tf.data.Dataset with the following Shapes:\n",
    "- x: (batches, window_length)\n",
    "- y: (batches,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def array_to_tf_data_api(train_data_x, train_data_y, batch_size=64, window_length=50,\n",
    "                         validate=False):\n",
    "    \"\"\"Applies sliding window on the fly by using the TF Data API.\n",
    "    Args:\n",
    "      train_data_x: Input Data as Numpy Array, Shape (rows, n_features)\n",
    "      batch_size: Batch Size.\n",
    "      window_length: Window Length or Window Size.\n",
    "      future_length: Number of time steps that will be predicted in the future.\n",
    "      n_output_features: Number of features that will be predicted.\n",
    "      validate: True if input data is a validation set and does not need to be shuffled\n",
    "      shift: Shifts the Sliding Window by this Parameter.\n",
    "    Returns:\n",
    "      tf.data.Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    X = tf.data.Dataset.from_tensor_slices(train_data_x)\n",
    "    y = tf.data.Dataset.from_tensor_slices(train_data_y)\n",
    "\n",
    "    if not validate:\n",
    "        train_tf_data = tf.data.Dataset.zip((X, y)).cache() \\\n",
    "            .shuffle(buffer_size=200000, reshuffle_each_iteration=True)\\\n",
    "            .batch(batch_size).prefetch(1)\n",
    "        return train_tf_data\n",
    "    else:\n",
    "        return tf.data.Dataset.zip((X, y)).batch(batch_size)\\\n",
    "            .prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom TF Callback to log Metrics by MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlflowLogging(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()  # handles base args (e.g., dtype)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        for key in keys:\n",
    "            mlflow.log_metric(str(key), logs.get(key), step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCategoricalCrossentropy(keras.losses.Loss):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.bce = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=False, reduction='sum')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, y_true, y_pred):\n",
    "        total = 0.0\n",
    "        for i in tf.range(y_pred.shape[1]):\n",
    "            loss = self.bce(y_true[:, i, 0], y_pred[:, i, :])\n",
    "            total = total + loss\n",
    "        return total\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config}\n",
    "\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalAccuracy(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"categorical_accuracy\", **kwargs):\n",
    "        super(CategoricalAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.true = self.add_weight(name=\"true\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
    "        self.accuracy = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, \"float32\")\n",
    "        y_pred = tf.cast(y_pred, \"float32\")\n",
    "\n",
    "        mask = y_true[:, :, 0] != 0\n",
    "        argmax = tf.cast(tf.argmax(y_pred, axis=2), \"float32\")\n",
    "        temp = argmax == y_true[:, :, 0]\n",
    "        true = tf.reduce_sum(tf.cast(temp[mask], dtype=tf.float32))\n",
    "        self.true.assign_add(true)\n",
    "        self.count.assign_add(\n",
    "            tf.cast(tf.shape(temp[mask])[0], dtype=\"float32\"))\n",
    "\n",
    "        self.accuracy.assign(tf.math.divide(self.true, self.count))\n",
    "\n",
    "    def result(self):\n",
    "        return self.accuracy\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.accuracy.assign(0.0)\n",
    "\n",
    "\n",
    "class CategoricalSessionAccuracy(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"categorical_session_accuracy\", **kwargs):\n",
    "        super(CategoricalSessionAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.true = self.add_weight(name=\"true\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
    "        self.accuracy = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.cast(y_true, \"float32\")\n",
    "        y_pred = tf.cast(y_pred, \"float32\")\n",
    "\n",
    "        mask = y_true[:, :, 0] != 0\n",
    "        argmax = tf.cast(tf.argmax(y_pred, axis=2), \"float32\")\n",
    "        temp = argmax == y_true[:, :, 0]\n",
    "        temp = tf.reduce_all(temp, axis=1)\n",
    "        true = tf.reduce_sum(tf.cast(temp, dtype=tf.float32))\n",
    "        self.true.assign_add(true)\n",
    "        self.count.assign_add(tf.cast(tf.shape(temp)[0], dtype=\"float32\"))\n",
    "\n",
    "        self.accuracy.assign(tf.math.divide(self.true, self.count))\n",
    "\n",
    "    def result(self):\n",
    "        return self.accuracy\n",
    "\n",
    "    def reset_states(self):\n",
    "        # The state of the metric will be reset at the start of each epoch.\n",
    "        self.accuracy.assign(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "4592/4592 [==============================] - 4794s 1s/step - loss: 4.9134 - categorical_accuracy: 0.5997 - categorical_session_accuracy: 0.4613\n",
      "Epoch 2/4\n",
      "4592/4592 [==============================] - 4665s 1s/step - loss: 3.6188 - categorical_accuracy: 0.5966 - categorical_session_accuracy: 0.4637\n",
      "Epoch 3/4\n",
      "4592/4592 [==============================] - 4664s 1s/step - loss: 3.1534 - categorical_accuracy: 0.6352 - categorical_session_accuracy: 0.5152\n",
      "Epoch 4/4\n",
      "4592/4592 [==============================] - 4663s 1s/step - loss: 3.1749 - categorical_accuracy: 0.6555 - categorical_session_accuracy: 0.5364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tmp/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tmp/assets\n",
      "2021/02/07 17:03:26 INFO mlflow.tensorflow: Validating the specified TensorFlow model by attempting to load it in a new TensorFlow graph...\n",
      "2021/02/07 17:04:11 INFO mlflow.tensorflow: Validation succeeded!\n",
      "Registered model 'Autoencoder for Session Based LSTM Recommender' already exists. Creating a new version of this model...\n",
      "2021/02/07 17:04:12 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: Autoencoder for Session Based LSTM Recommender, version 8\n",
      "Created version '8' of model 'Autoencoder for Session Based LSTM Recommender'.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as parent_run:\n",
    "    for params in grid_search_param:\n",
    "        batch_size = params['batch_size']\n",
    "        window_length = params['window_length']\n",
    "        embedding_dim = params['embedding_dim']\n",
    "        dropout_fc = params['dropout_fc']\n",
    "        hidden_layer_size = params['hidden_layer_size']\n",
    "        n_output_features = params['n_output_features']\n",
    "        n_input_features = params['n_input_features']\n",
    "\n",
    "        with mlflow.start_run(nested=True) as child_run:\n",
    "            # log parameter\n",
    "            mlflow.log_param('batch_size', batch_size)\n",
    "            mlflow.log_param('window_length', window_length)\n",
    "            mlflow.log_param('hidden_layer_size', hidden_layer_size)\n",
    "            mlflow.log_param('dropout_fc_layer', dropout_fc)\n",
    "            mlflow.log_param('embedding_dim', embedding_dim)\n",
    "            mlflow.log_param('n_output_features', n_output_features)\n",
    "            mlflow.log_param('n_unique_input_ids', n_unique_input_ids)\n",
    "            mlflow.log_param('n_input_features', n_input_features)\n",
    "\n",
    "            model = build_autoencoder(window_length=window_length,\n",
    "                                             n_output_features=n_output_features,\n",
    "                                             n_unique_input_ids=n_unique_input_ids,\n",
    "                                             n_input_features=n_input_features,\n",
    "                                             embedding_dim=embedding_dim,\n",
    "                                             units_lstm_layer=hidden_layer_size,\n",
    "                                             dropout_rate=dropout_fc)\n",
    "\n",
    "            data = array_to_tf_data_api(sessions_padded,\n",
    "                                        sessions_padded,\n",
    "                                        window_length=window_length,\n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "            model.compile(loss=CustomCategoricalCrossentropy(),#tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='sum'),\n",
    "                          optimizer=keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "                          metrics=[CategoricalAccuracy(), CategoricalSessionAccuracy()])\n",
    "\n",
    "            model.fit(data, shuffle=True, initial_epoch=0, epochs=2,\n",
    "                      callbacks=[MlflowLogging()])\n",
    "            \n",
    "            model.compile()\n",
    "            model.save(\"./tmp\")\n",
    "            model.save_weights('weights')\n",
    "\n",
    "            mlflow.tensorflow.log_model(tf_saved_model_dir='./tmp',\n",
    "                                        tf_meta_graph_tags='serve',\n",
    "                                        tf_signature_def_key='serving_default',\n",
    "                                        artifact_path='saved_model',\n",
    "                                        registered_model_name='Session Based LSTM Recommender')\n",
    "\n",
    "            shutil.rmtree(\"./tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
